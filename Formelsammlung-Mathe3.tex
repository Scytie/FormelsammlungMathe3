\documentclass[10pt,twocolumn,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[german]{babel}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{color}
\usepackage{enumitem}
\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}

\setlist{noitemsep}
\setlist[itemize]{leftmargin=*}
\setlist[itemize,1]{label=-}

\begin{document}
\begin{flushleft}

\textbf{Permutation}

Berücksichtigung der Reihenfolge $k=n$\\
ohne Wdh/Zl: $n!$\\
mit Wdh/Zl: $\frac{n!}{k!}$\\

\textbf{Variation}

Berücksichtigung der Reihenfolge $k < n$\\
ohne Wdh/Zl: $\frac{n!}{(n-k)!}$\\
mit Wdh/Zl: $n^k$\\

\textbf{Kombination}

Ohne Berücksichtigung der Reihenfolge $k < n$\\
ohne Wdh/Zl: $\binom{n}{k}$\\
mit Wdh/Zl: $\binom{n+k-1}{k}$\\

\textbf{Wahrscheinlichkeiten}

ZFE: bel. oft, gleiche Bed., Erg. nicht vorhersagbar\\
Ereignis $A \subseteq \Omega$; Laplace: $P(A) = \frac{|A|}{|\Omega|}$\\

$P(A \cup B) = P(A) + P(B) - P(A \cap B)$\\
A und B unabhängig: $P(A \cap B) = P(A) \cdot P(B)$\\
$P(A|B) = \frac{P(A \cap B)}{P(B)}$\\

\textbf{Satz der totalen Wahrscheinlichkeit}

$P(B) = \sum_{i=1}^m P(A_i) P(B|A_i) = \sum_{i=1}^m P(A_i \cap B)$\\

\textbf{Satz von Bayes}

$P(A_k|B) = \frac{P(A)}{P(B)} \cdot P(B|A_k) = \frac{P(A_k) P(B|A_k)}{\sum_{i=1}^m P(A_i) P(B|A_i)}$\\

\textbf{Zufallsvariablen}

$X: \Omega \rightarrow R$; Realisierungen: $\{ X(\omega) : \omega \in \Omega\}$\\
$P(X=k) = P(X(\omega)=k)$\\

\textbf{Dichtefunktion $f: \mathbb{R} \rightarrow \mathbb{R}$}

(1) $\forall x \in \mathbb{R}: f(x) \geq 0$; (2) f integrierbar\\
(3) $\int_{-\infty}^{\infty}f(x)dx = 1$\\

\textbf{Verteilungsfunktion}

$F(t) = P({\omega \in \Omega: X(\omega \leq t)}) = P(X \leq t)$\\
(1) mon. wachsend (2) $lim_{x\searrow x^*}F(x)=F(x^*)$\\
(3) $lim_{x\searrow x^*}F(x)-lim_{x\nearrow x^*}F(x)=P(X=x^*)$\\
$P(x < X \leq y) = F(y) - F(x)$\\
$F(t) = \int_{-\infty}^t f(x) dx$\\

\textbf{Quantil}

$lim_{x\nearrow Q_p}F(x) \leq p \leq F(Q_p)$\\

\textbf{Binomialkoeffizienten}

$\binom{n}{k} = \frac{n!}{k!(n-k)!}$; $0! = 1$\\
$\binom{n}{k} = 0$ für k>n\\
$\binom{n}{0} = \binom{n}{n} = 1$; $\binom{n}{1} = n$\\

\textbf{Binomialverteilung B(n,p)}

$P(X=k) = \binom{n}{k}p^k(1-p)^{n-k}$\\
$\mu = np$\\
$\sigma = \sqrt{np(1-p)}$\\
$P(X=k) \approx \frac{\lambda^k}{k!}e^{-\lambda}$ für $n \geq 50 \wedge p \leq 0,1$; $\lambda = np$\\
$F(t) \approx \Phi(\frac{t+0,5-\mu}{\sigma})$ bei $\sigma^2 = np(1-p) \geq 9$

\textbf{Hypergeometrische Verteilung H(n, M, N)}

$P(X=k) = \frac{\binom{M}{k}\binom{N-M}{n-k}}{\binom{N}{n}}$\\
n: Anz. ziehen; M: ges Eig.; N: gesamt\\
$E(X) = n\frac{M}{N}$\\
Apprx durch B bei $n/N < 0,05$\\

\textbf{Poisson-Verteilung $P_\lambda(k)$}

$P(X=k) = \frac{\lambda^k}{k!}e^{-\lambda}$\\
$E(X) = V(X) = \lambda$\\

\textbf{Erwartungswert}

$E(X) = \sum_{k \in X} x \cdot P(X=k)$\\
$E(X) = \int_{-\infty}^{\infty} xf(x) dx$\\
$E(X + \alpha Y) = E(X) + \alpha E(Y)$\\
X,Y unabh: $E(X \cdot Y) = E(X) \cdot E(Y)$\\

\textbf{Varianz $\sigma^2$}

$Var(X) = E(X^2) - E(X)^2$\\
$Var(X+Y) = Var(X) + Var(Y) + 2 Cov(X,Y)$\\
X,Y unabh: $Var(XY) = E(X^2)E(Y^2) - E(X)^2E(Y)^2$\\

\textbf{Kovarianz}

$Cov(X, Y) = E(XY) - E(X)E(Y)$\\
X,Y unabh: $Cov(X,Y) = 0$\\

\textbf{Normalverteilung $N(\mu, \sigma^2)$}

$F(x) = \Phi(\frac{x-\mu}{\sigma})$\\
$\Phi(-t) = 1-\Phi(t)$

\textbf{Gleichverteilung auf [a,b]}

$f(x) = \frac{1}{b-a}$ für $x \in (a,b)$ sonst 0\\
$E(X) = \frac{b+a}{2}$\\
$V(X) = \frac{(b-a)^2}{12}$

\textbf{Exponentialverteilung $Exp(\lambda)$}

$f(x) = \lambda e^{-\lambda x}$ für $x \geq 0$ sonst 0\\
$F(t) = 1-e^{-\lambda t}$ für $t \geq 0$ sonst 0\\
$E(X) = \frac{1}{\lambda}$; $V(X) = \frac{1}{\lambda^2}$\\

\textbf{Stichprobengrößen}

$\bar{x} = \frac{1}{n} \sum_{i=1}^{n}x_i$\\
$\bar{s}_x^2 = \frac{1}{n-1} \sum_{i=1}^{n}(x_i-\bar{x})^2$\\
emp. Korrelation: $s_{x,y} = \frac{1}{n-1} \sum_{i=1}^{n}(x_i-\bar{x})(y_i-\bar{y})$\\

\textbf{Konfidenzintervalle}

$\Phi(z_{\alpha}) = \alpha$; $\chi^2_n(c_{n-1,\alpha})=\alpha$\\
$[\bar{X}-\sigma z_{1-\alpha/2}/\sqrt{n}, \bar{X}+\sigma z_{1-\alpha/2}/\sqrt{n}]$ (für $\mu)$\\
$(-\infty, \bar{X}+\sigma z_{\textcolor{red}{1-\alpha}}/\sqrt{n}]$ (einseitig für $\mu)$\\
$[\bar{X} - t_{n-1,1-\alpha/2}\bar{S}/\sqrt{n}, \bar{X} + ...]$ (für $\mu$ ohne $\sigma$)\\
$[\frac{(n-1)\bar{S}^2}{c_{n-1,1-\alpha/2}},\frac{(n-1)\bar{S}^2}{c_{n-1,\textcolor{red}{\alpha/2}}}]$ (für Varianz)\\
$[\frac{k}{n}-\frac{z_{1-\alpha/2}}{\sqrt{n}}\sqrt{\frac{k}{n}(1-\frac{k}{n})}, \frac{k}{n}+..]$ (für B mit k,n-k>50)\\

\textbf{Hypothesentest Erwartungswert}

$H_0: \mu > \mu_0$; $\bar{H_0}: \mu \leq \mu_0$\\
$P(A_0|\bar{H_0}) = P(X \geq k) = 1 - \Phi(\frac{k-\mu_0}{\sigma / \sqrt{n}}) \leq \alpha$\\

\textbf{Hypothesentest Varianz}

$H_0: \sigma^2 > \sigma_0^2$; $\bar{H_0}: \sigma^2 \leq \sigma_0^2$\\
$P(A_0|\bar{H_0}) = P(X \geq k) = 1 - \chi^2_{n-1}(\frac{k(n-1)}{\sigma_0^2}) \leq \alpha$\\

\textbf{Lagrange-Interpolation}

$L_i(x) = \prod_{j=0;j \ne i}^n \frac{x-x_j}{x_i-x_j}$\\
$P_n(x) = \sum_{i=0}^n y_i L_i(x)$\\

\textbf{Newton-Interpolation}

$P_n(x) = \sum_{i=0}^n \alpha_{i,i} \prod_{j=0}^{i-1}(x-x_j)$\\
$\alpha_{i,0} = y_i$; $\alpha_{i,j} = \frac{\alpha_{i,j-1} - \alpha_{i-1,j-1}}{x_i - x_{i-j}}$\\

\textbf{Neville-Interpolation}

$P_{i,0}(x) = y_i$\\
$P_{i,j}(x) = \frac{(x-x_{i-j})P_{i,j-1}(x) - (x-x_i)P_{i-1,j-1}(x)}{x_i - x_{i-j}}$\\

\textbf{Newton-Cotes-Formel}

$\int_a^b f(x) dx \approx h \sum_{i=0}^n \alpha_i f(x_i)$ mit $h=\frac{b-a}{h}$; $x_i = a + hi$\\
$\alpha_i = \int_0^n \Pi_{j=0;j\neq i}^n \frac{t-j}{i-j} dt$\\

\textbf{Newtonverfahren}

$x_{n+1} = x_n - \frac{f(x_n)}{f'(x_n)}$\\
$x_{n+1} = x_n - \frac{x_n - x_{n-1}}{f(x_n) - f(x_{n-1})} \cdot f(x_n)$\\

\textbf{strikt diagonaldominant}

$\forall i = 1,..,n: |a_{\textcolor{red}{ii}}| > \sum_{j=1,j\neq i}^n |a_{ij}|$\\
J- und G-S-Verfahren konvergieren hier gegen Lösung\\

\textbf{Normen}

Zeilensummennorm:\\
$||A||_\infty = max_{||x||_\infty=1}||Ax||_\infty=max_{i=1..m}\sum_{j=1}^n |a_{ij}|$\\
Spaltensummennorm:\\
$||A||_1 = max_{||x||_1=1}||Ax||_1=max_{j=1..m}\sum_{i=1}^n |a_{ij}|$\\
Spektralnorm:\\
$||A||_2 = max_{||x||_2=1}||Ax||_2=...$\\
Submultiplikativität:\\
$||AB|| \leq ||A|| \cdot ||B||$\\
Verträglichkeit mit Vektornorm:\\
$||Ax||_V \leq ||A|| \cdot ||x||_V$\\


\textbf{Jacobiverfahren}

$A = L + U + D$ L: Lower, U: Upper, D: Diag.\\
$x_{k+1} := D^{-1}(b-(L+U)x_k)$\\

\textbf{Gauß-Seidel-Verfahren}

$x_{k+1} = (D+L)^{-1} (b-Ux_k)$\\

\textbf{LU-Faktorisierung}

$A = PLU$, L: untere DM mit 1 auf Diag., P: Permut.\\
$U_0 = A$, $L_0 = E_n$\\
Pivot: \\

\textbf{Cholesky-Zerlegung}

A muss symmetrisch und positiv definit sein\\
in Praxis: symmetrisch und dann probieren, ob Verfahren funktioniert\\
$A = GG^T$, G - untere Dreiecksmatrix\\
$g_{jj} = \sqrt{a_{jj} - \sum_{k=1}^{j-1} g_{jk}^2}$\\
$g_{ij} = \frac{1}{g_{jj}} (a_{ij} - \sum_{k=1}^{j-1} g_{ik}g_{jk}), i=j+1,..,n$\\
zeilenweise von links nach rechts\\
Lösen GS: $GG^Tx=b$, $y:=G^Tx$\\
1) y aus $Gy=b$ 2) x aus $G^Tx=y$\\

\textbf{QR-Zerlegung}

$A = QR$\\
$u_i = a_i - \sum_{j=1}^{i-1} \frac{<u_j, a_{\textcolor{red}{i}}>}{||u_j||^2}u_j$\\
$q_i = u_i / ||u_i||$, $r_{ij} = \frac{<u_i, a_j>}{||u_i||}, 1 \leq i \leq j \leq n$\\

\textbf{Separation der Variablen}

Umstellen, dass alle x auf einer Seite und y auf der anderen. Dann integrieren.

\textbf{Lineare DGL 1. Ordnung}

$y'(x)=a_0(x)y(x) \Rightarrow y(x) = ce^{\int{a_0}dx}$\\

\textbf{Variation der Konstanten}

(1) Homogene DGL lösen (2) c -> k(x); y(k(x)) differenzieren (3) y, y' in inhomogene DGL einsetzen -> k(x)\\

\textbf{D'Alembert'sches Reduktionsverfahren}

$y_1$ als spezielle Lösung gegeben\\
$y=y_1 \cdot u(x)$ als Ansatz\\
$z(x) = u'(x)$\\
$y_2 = y_1 \cdot u(x)$\\

\textbf{Lineare DGL 2. Ordnung mit konst Koeff}

zwei NST: $y = c_1 e^{\lambda_1 x} + c_2 e^{\lambda_2 x}$\\
eine NST: $y = c_1 e^{\lambda x} + c_2 \textcolor{red}{x} e^{\lambda x}$\\

\textbf{Variation der Konstanten bei 2. Ordnung}

NB: $c_1'y_1 + c_2'y_2 = 0$\\
$W_1(x) = \int - \frac{y_2(x)b(x)}{W(x)} dx$\\
$W_2(x) = \int \frac{y_1(x) b(x)}{W(x)} dx$\\
$y_p = W_1(x)y_1(x) + W_2(x)y_2(x)$\\

\textbf{Wronski-Determinante}

$ W(f_1,..,f_n)(x) =
\begin{vmatrix}
    f_{1}(x) & \dots  & x_{n}(x) \\
    \vdots & \ddots & \vdots \\
    f_{1}^{(n-1)}(x) & \dots & f_n^{(n-1)}(x)
\end{vmatrix}, x \in I
$\\
$\exists x_0 \in I: W(f_1,..,f_n)(x_0) \neq 0 \Rightarrow f_1,..,f_n$ lin. unabh.; umgekehrt nicht\\

\textbf{Differenzieren und Integrieren}

$\frac{d}{{dx}}x^n  = nx^{\left( {n - 1} \right)}$\\
$\frac{d}{{dx}}e^{ax}  = ae^{ax}$\\
$\frac{d}{{dx}}b^x  = b^x \ln \left( b \right)$\\
$\frac{d}{{dx}}\ln \left( x \right) = \frac{1}{x}$\\
$\frac{d}{{dx}}\sin x = \cos x$\\
$\frac{d}{{dx}}\cos x =  - \sin x$\\

$\int {x^n } dx = \frac{{x^{n + 1} }}{{n + 1}},(n \ne  - 1)$\\
$\int {\frac{1}{x}} dx = \ln \left| x \right| + c$\\
$\int {\cos (ax)} dx = \frac{1}{a}\sin (ax) + c$\\
$\int {\sin (ax)} dx =  - \frac{1}{a}\cos (ax) + c$\\
$\int {\ln (x)} dx = x ln(x) -x$\\

\textbf{Regeln Differenzieren}

$\frac{d}{{dx}}\left( {f\left( x \right)g\left( x \right)} \right) = f\left( x \right)g'\left( x \right) + f'\left( x \right)g\left( x \right)$\\
$\frac{d}{{dx}}\left( {\frac{{f\left( x \right)}}{{g\left( x \right)}}} \right) = \frac{{f'\left( x \right)g\left( x \right) - f\left( x \right)g'\left( x \right)}}{{g^2 \left( x \right)}}$\\
$\frac{d}{{dx}}\left[ {f\left( u \right)} \right] = \frac{d}{{du}}\left[ {f\left( u \right)} \right]\frac{{du}}{{dx}}$\\

\textbf{Regeln Integrieren}

$\int {uv'} dx = uv - \int {u'v}dx$\\

\textbf{Euler Formel}

$e^{ix} = \cos(x) + i \sin(x)$
$e^{-ix} = \cos(x) - i \sin(x)$

\textbf{Winkelbeziehungen}

$cos(-x) = cos(x)$; $sin(-x) = -sin(x)$\\
$\sin \left( {\theta _1  \pm \theta _2 } \right) = \sin \theta _1 \cos \theta _2  \pm \cos \theta _1 \sin \theta _2$\\
$\cos \left( {\theta _1  \pm \theta _2 } \right) = \cos \theta _1 \cos \theta _2  \mp \sin \theta _1 \sin \theta _2$\\

\textbf{Potenzgesetze}

$x^a x^b  = x^{\left( {a + b} \right)}$\\
$x^a y^a  = \left( {xy} \right)^a$\\
$\left( {x^a } \right)^b  = x^{\left( {ab} \right)}$\\

\textbf{Logarithmengesetze}

$y = \log _b \left( x \right) \Leftrightarrow x = b^y$\\
$\log _b \left( 1 \right) = 0$; $\log _b \left( b \right) = 1$\\
$\log _b \left( {xy} \right) = \log _b \left( x \right) + \log _b \left( y \right)$\\
$\log _b \left( {\frac{x}{y}} \right) = \log _b \left( x \right) - \log _b \left( y \right)$\\
$\log _b \left( {x^n } \right) = n\log _b \left( x \right)$\\
$\log _b \left( x \right) = \log _b \left( c \right)\log _c \left( x \right) = \frac{{\log _c \left( x \right)}}{{\log _c \left( b \right)}}$\\

\textbf{Lineare Unabhängigkeit}

\textbf{Werte von Sinus/Cosinus}

\begin{tabular}{|c|c|c|c|}
\hline
\rule[-1ex]{0pt}{2.5ex} x & sin(x) & cos(x) & tan(x) \\
\hline
\rule[-1ex]{0pt}{2.5ex} $0^\circ$ & $0$ & $1$ & $0$ \\
\hline
\rule[-1ex]{0pt}{2.5ex} $30^\circ$ & $\frac{1}{2}$ & $\frac{\sqrt{3}}{2}$ & $\frac{\sqrt{3}}{3}$ \\
\hline
\rule[-1ex]{0pt}{2.5ex} $45^\circ$ & $\frac{\sqrt{2}}{2}$ & $\frac{\sqrt{2}}{2}$ & $1$ \\
\hline
\rule[-1ex]{0pt}{2.5ex} $60^\circ$ & $\frac{\sqrt{3}}{2}$ & $\frac{1}{2}$ & $\sqrt{3}$ \\
\hline
\rule[-1ex]{0pt}{2.5ex} $90^\circ$ & $1$ & $0$ & - \\
\hline
\rule[-1ex]{0pt}{2.5ex} $120^\circ$ & $\frac{\sqrt{3}}{2}$ & $-\frac{1}{2}$ & $-\sqrt{3}$ \\
\hline
\rule[-1ex]{0pt}{2.5ex} $135^\circ$ & $\frac{\sqrt{2}}{2}$ & $-\frac{\sqrt{2}}{2}$ & $-1$ \\
\hline
\rule[-1ex]{0pt}{2.5ex} $150^\circ$ & $\frac{1}{2}$ & $-\frac{\sqrt{3}}{2}$ & $-\frac{\sqrt{3}}{3}$ \\
\hline
\rule[-1ex]{0pt}{2.5ex} $180^\circ$ & $0$ & $-1$ & $0$ \\
\hline
\rule[-1ex]{0pt}{2.5ex} $225^\circ$ & $-\frac{\sqrt{2}}{2}$ & $-\frac{\sqrt{2}}{2}$ & $1$ \\
\hline
\rule[-1ex]{0pt}{2.5ex} $270^\circ$ & $-1$ & $0$ & - \\
\hline
\end{tabular}


\end{flushleft}
\end{document}
